{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torcheval.metrics import AUC\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os, cv2\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from metrics_compute_plot import PR_plot_CV, binary_auc_plot_cv\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import SimpleITK as sitk\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,ShuffleSplit\n",
    "from ctviewer import CTViewer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的 GPU 设备\n",
    "if torch.cuda.is_available():\n",
    "    # 设置 PyTorch 使用的 GPU 设备为卡1\n",
    "    torch.cuda.set_device(0)  # 使用 GPU 卡1\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    # 如果没有可用的 GPU，则使用 CPU\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Methods\n",
    "## Data Loader with batchsize in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "plt.ion()  # interact\n",
    "\n",
    "class PrefetchDataLoader(torch.utils.data.DataLoader):\n",
    "    '''\n",
    "        replace DataLoader with PrefetchDataLoader\n",
    "    '''\n",
    "    def __iter__(self):\n",
    "        return BackgroundGenerator(super().__iter__())\n",
    "def label_smooth(label, C, epsilon, is_onehot=True):\n",
    "    \"\"\"\n",
    "    Smooths the labels, commonly used in machine learning and deep learning to address label noise issues.\n",
    "    Args:\n",
    "        label: Labels, of type np.ndarray, with shape (batch_size, C)\n",
    "        C: Number of classes\n",
    "        epsilon: Smoothing factor, should be greater than or equal to 0 and less than or equal to 1\n",
    "        is_onehot: Whether the labels are in one-hot encoding, default is True\n",
    "    Returns:\n",
    "        smooth_labels: Smoothed labels, of type np.ndarray, with the same shape as label\n",
    "    \"\"\"\n",
    "    assert epsilon >= 0.0 and epsilon <= 1.0, \"epsilon should be in [0.0, 1.0]\"\n",
    "    confidence = 1.0 - epsilon\n",
    "    #print(label.shape)\n",
    "    # one-hot label\n",
    "    if label.shape[-1] == C and is_onehot is True:\n",
    "        smooth_labels = label * confidence + epsilon / C\n",
    "\n",
    "    # index label\n",
    "    else:\n",
    "        # Convert index labels to one-hot labels\n",
    "        eye_matrix = np.eye(C)\n",
    "        onehot_labels = eye_matrix[label]\n",
    "        smooth_labels = onehot_labels * confidence + epsilon / C\n",
    "    return smooth_labels#smooth_labels.astype(np.float64)\n",
    "\n",
    "class CELoss_mixup(nn.Module):\n",
    "    ''' Cross Entropy Loss with label smoothing and mix up'''\n",
    "    def __init__(self, label_smooth=None, class_num=2, mix_up = None):\n",
    "        super().__init__()\n",
    "        self.label_smooth = label_smooth\n",
    "        self.class_num = class_num\n",
    "        self.mix_up = mix_up\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        ''' \n",
    "        Args:\n",
    "            pred: prediction of model output    [N, M]\n",
    "            target: ground truth of sampler [N]\n",
    "        '''\n",
    "        eps = 1e-12\n",
    "        \n",
    "        if self.label_smooth is not None:\n",
    "            # cross entropy loss with label smoothing\n",
    "            logprobs = F.log_softmax(pred, dim=1)\t# softmax + log\n",
    "            target = F.one_hot(target, self.class_num)\t# 转换成one-hot\n",
    "            \n",
    "            # label smoothing\n",
    "            # 实现 1\n",
    "            # target = (1.0-self.label_smooth)*target + self.label_smooth/self.class_num \t\n",
    "            # 实现 2\n",
    "            # implement 2\n",
    "            target = torch.clamp(target.float(), min=self.label_smooth/(self.class_num-1), max=1.0-self.label_smooth)\n",
    "            loss = -1*torch.sum(target*logprobs, 1)\n",
    "        elif self.mix_up is not None:\n",
    "            # cross entropy loss with mix up\n",
    "            logprobs = F.log_softmax(pred, dim=1)\t# softmax + log\n",
    "            \n",
    "            # label smoothing\n",
    "            loss = -1*torch.sum(target*logprobs, 1)\n",
    "            \n",
    "        else:\n",
    "            # standard cross entropy loss\n",
    "            loss = -1.*pred.gather(1, target.unsqueeze(-1)) + torch.log(torch.exp(pred+eps).sum(dim=1))\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class Dataset_loader(torch.utils.data.Dataset):\n",
    "    \"\"\"dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): 带有标注信息的 csv 文件路径\n",
    "            transform (callable, optional): 可选的用于预处理图片的方法\n",
    "        \"\"\"\n",
    "        self.dataframe = df_file#pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 读取图片\n",
    "        depth = 9\n",
    "        image1 = sitk.GetArrayFromImage(sitk.ReadImage('/home/u20210110/jupyterlab/HAIC_TACE/Processed_data_cut/'+str(self.dataframe.iloc[idx, 3])+'/'+os.path.basename(self.dataframe.iloc[idx, 2])+ '/AP.nii.gz'))#修改文件路径\n",
    "        #image2 = sitk.GetArrayFromImage(sitk.ReadImage('/home/u20210110/jupyterlab/HAIC_TACE/Processed_data_cut/'+str(self.dataframe.iloc[idx, 0])+ '/VP.nii.gz'))#修改文件路径\n",
    "        steps = len(image1)//depth\n",
    "        #print(str(self.dataframe.iloc[idx, 3])+'/'+str(self.dataframe.iloc[idx, 1])+':'+str(steps))\n",
    "        '''\n",
    "        if steps>0:\n",
    "            if np.mod(len(image1),depth)==0:\n",
    "                image1 = image1[::steps,:,:]\n",
    "            else:\n",
    "                image1 = image1[steps::steps,:,:]\n",
    "        image1 = resize(image1,(depth,224,224))\n",
    "        '''\n",
    "        if steps>0:\n",
    "            idx_slice = len(image1) // 2\n",
    "            image1 = image1[idx_slice-depth//2:idx_slice+depth//2,:,:]\n",
    "        image1 = resize(image1,(depth,224,224))    \n",
    "        image1 = image1.reshape(1,depth*224,224)\n",
    "        #steps = len(image2)//depth\n",
    "        #if np.mod(len(image2),depth)==0:\n",
    "            #image2 = image2[::steps,:,:]\n",
    "        #else:\n",
    "            #image2 = image2[steps::steps,:,:]\n",
    "        #image2 = resize(image2,(depth,224,224))\n",
    "        #image = np.concatenate((image1,image2),axis=-3)\n",
    "        image = ((image1-0.)/255)\n",
    "        \n",
    "        # 读取标签\n",
    "        label = self.dataframe.iloc[idx, -8]\n",
    "        #label_time = self.dataframe.iloc[idx, -3]\n",
    "        #label_state = self.dataframe.iloc[idx, -4]\n",
    "        label = np.array([label])\n",
    "        #label_time = np.array([label_time])\n",
    "        #label_state = np.array([label_state])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # sample = {'image': image, 'label': label}\n",
    "        return torch.Tensor(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# an alternative for mamba_ssm (in which causal_conv1d is needed)\n",
    "try:\n",
    "    from selective_scan import selective_scan_fn as selective_scan_fn_v1\n",
    "    from selective_scan import selective_scan_ref as selective_scan_ref_v1\n",
    "except:\n",
    "    pass\n",
    "\n",
    "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size1=4,patch_size2=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size1, int) and isinstance(patch_size2, int):\n",
    "            patch_size = (patch_size1, patch_size2)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging2D(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "        \n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SS2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        # d_state=\"auto\", # 20240109\n",
    "        d_conv=3,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        dropout=0.,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            groups=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            padding=(d_conv - 1) // 2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = (\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), \n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), \n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), \n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), \n",
    "        )\n",
    "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)\n",
    "        del self.x_proj\n",
    "\n",
    "        self.dt_projs = (\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "        )\n",
    "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)\n",
    "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)\n",
    "        del self.dt_projs\n",
    "        \n",
    "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "\n",
    "        # self.selective_scan = selective_scan_fn\n",
    "        self.forward_core = self.forward_corev0\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
    "\n",
    "    @staticmethod\n",
    "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):\n",
    "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        dt_proj.bias._no_reinit = True\n",
    "        \n",
    "        return dt_proj\n",
    "\n",
    "    @staticmethod\n",
    "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        if copies > 1:\n",
    "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
    "            if merge:\n",
    "                A_log = A_log.flatten(0, 1)\n",
    "        A_log = nn.Parameter(A_log)\n",
    "        A_log._no_weight_decay = True\n",
    "        return A_log\n",
    "\n",
    "    @staticmethod\n",
    "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
    "        # D \"skip\" parameter\n",
    "        D = torch.ones(d_inner, device=device)\n",
    "        if copies > 1:\n",
    "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
    "            if merge:\n",
    "                D = D.flatten(0, 1)\n",
    "        D = nn.Parameter(D)  # Keep in fp32\n",
    "        D._no_weight_decay = True\n",
    "        return D\n",
    "\n",
    "    def forward_corev0(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1) # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts, \n",
    "            As, Bs, Cs, Ds, z=None,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=False,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    # an alternative to forward_corev1\n",
    "    def forward_corev1(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn_v1\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1) # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts, \n",
    "            As, Bs, Cs, Ds,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.act(self.conv2d(x)) # (b, d, h, w)\n",
    "        y1, y2, y3, y4 = self.forward_core(x)\n",
    "        assert y1.dtype == torch.float32\n",
    "        y = y1 + y2 + y3 + y4\n",
    "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
    "        y = self.out_norm(y)\n",
    "        y = y * F.silu(z)\n",
    "        out = self.out_proj(y)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvSSM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 0,\n",
    "        drop_path: float = 0,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        attn_drop_rate: float = 0,\n",
    "        d_state: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(hidden_dim//2)\n",
    "        self.self_attention = SS2D(d_model=hidden_dim//2, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "        self.conv33conv33conv11 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_dim//2,out_channels=hidden_dim//2,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim // 2, out_channels=hidden_dim // 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim // 2, out_channels=hidden_dim // 2, kernel_size=1, stride=1)\n",
    "        )\n",
    "        self.finalconv11 = nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, stride=1)\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        input_left, input_right = input.chunk(2,dim=-1)\n",
    "        x = input_right + self.drop_path(self.self_attention(self.ln_1(input_right)))\n",
    "        input_left = input_left.permute(0,3,1,2).contiguous()\n",
    "        input_left = self.conv33conv33conv11(input_left)\n",
    "        x = x.permute(0,3,1,2).contiguous()\n",
    "        output = torch.cat((input_left,x),dim=1)\n",
    "        output = self.finalconv11(output).permute(0,2,3,1).contiguous()\n",
    "        return output+input\n",
    "\n",
    "\n",
    "class VSSLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        depth, \n",
    "        attn_drop=0.,\n",
    "        drop_path=0., \n",
    "        norm_layer=nn.LayerNorm, \n",
    "        downsample=None, \n",
    "        use_checkpoint=False, \n",
    "        d_state=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConvSSM(\n",
    "                hidden_dim=dim,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "                attn_drop_rate=attn_drop,\n",
    "                d_state=d_state,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
    "            def _init_weights(module: nn.Module):\n",
    "                for name, p in module.named_parameters():\n",
    "                    if name in [\"out_proj.weight\"]:\n",
    "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
    "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "            self.apply(_init_weights)\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x,use_reentrant=False)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class VSSM(nn.Module):\n",
    "    def __init__(self, patch_size1=4,patch_size2=4, in_chans=3, num_classes=1000, depths=[2, 2, 2, 2], depths_decoder=[2, 9, 2, 2],\n",
    "                 dims=[96, 192, 384, 768], dims_decoder=[768, 384, 192, 96], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes,sum(dims[1:])+dims[-1]))\n",
    "        #print(self.prototypes.shape)\n",
    "        self.num_layers = len(depths)\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
    "        self.embed_dim = dims[0]\n",
    "        self.num_features = sum(dims[1:])+dims[-1]\n",
    "        self.dims = dims\n",
    "\n",
    "        self.patch_embed = PatchEmbed2D(patch_size1=patch_size1,patch_size2=patch_size2, in_chans=in_chans, embed_dim=self.embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None)\n",
    "\n",
    "        # WASTED absolute position embedding ======================\n",
    "        self.ape = False\n",
    "        # self.ape = False\n",
    "        # drop_rate = 0.0\n",
    "        if self.ape:\n",
    "            self.patches_resolution = self.patch_embed.patches_resolution\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "        dpr_decoder = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths_decoder))][::-1]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = VSSLayer(\n",
    "                dim=dims[i_layer],\n",
    "                depth=depths[i_layer],\n",
    "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
    "                drop=drop_rate, \n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "\n",
    "        # self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    def _init_weights(self, m: nn.Module):\n",
    "        \"\"\"\n",
    "        out_proj.weight which is previously initilized in ConvSSM, would be cleared in nn.Linear\n",
    "        no fc.weight found in the any of the model parameters\n",
    "        no nn.Embedding found in the any of the model parameters\n",
    "        so the thing is, ConvSSM initialization is useless\n",
    "        \n",
    "        Conv2D is not intialized !!!\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def compute_distances(self, x):\n",
    "        distances = torch.norm(x.unsqueeze(1) - self.prototypes, dim=2)\n",
    "        return distances#torch.argmin(distances, dim=1)\n",
    "    \n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        mutiple_ouput = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            mutiple_ouput.append(x)\n",
    "        return mutiple_ouput\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_backbone(x)\n",
    "        mutiple = x[0].permute(0,3,1,2)\n",
    "        mutiple = self.avgpool(mutiple)\n",
    "        #print(mutiple.shape)\n",
    "        for single in x[1:]:\n",
    "            single = single.permute(0,3,1,2)\n",
    "            single = self.avgpool(single)\n",
    "            #print(single.shape)\n",
    "            mutiple = torch.cat((mutiple,single),dim=1)\n",
    "        x = torch.flatten(mutiple,start_dim=1)\n",
    "        x = self.pos_drop(x)\n",
    "        #print(x.shape)\n",
    "        distances = self.compute_distances(x)\n",
    "        x = self.head(x)\n",
    "        return distances,x\n",
    "\n",
    "\n",
    "# model = VSSM(num_classes=6).to(\"cuda\")\n",
    "#\n",
    "# data = torch.randn(1,3,224,224).to(\"cuda\")\n",
    "#\n",
    "# print(model(data).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelearte with data load\n",
    "## Train model function with dataset all in memory for AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(f'/home/u20210110/jupyterlab/HAIC_TACE/Final_V7-Selected.xlsx')\n",
    "idx = (df['Phase_A']==1)&(df['Phase_V']==1)&(pd.isna(df['proliferative']))\n",
    "df = df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(np.unique(df['Center'])):\n",
    "    print(f\"{i} has data {np.sum(df['Center']==i)}: HAIC {np.sum((df['Center']==i)&(df['HAIC/TACE']==1))}, TACE {np.sum((df['Center']==i)&(df['HAIC/TACE']==0))}\")\n",
    "print(f\"ALL data {len(df['Center'])}: HAIC {np.sum(df['HAIC/TACE']==1)}, TACE {np.sum(df['HAIC/TACE']==0)}\")\n",
    "print(f'Train HAIC {396+129+442+21}, TACE {473+138}')\n",
    "print(f'test HAIC {156+50+3+38}, TACE {28+33+12+132+10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['最大肿瘤直径']=='不易测量','最大肿瘤直径'] = 0\n",
    "df = df[(df['HAIC/TACE']==0)&(df['最大肿瘤直径']>=5)]\n",
    "#OR\n",
    "df.loc[(df['疗效评估 (PD or PR or CR)']=='CR')|(df['疗效评估 (PD or PR or CR)']=='CR ')|(df['疗效评估 (PD or PR or CR)']=='PR')|(df['疗效评估 (PD or PR or CR)']=='pR')|(df['疗效评估 (PD or PR or CR)']=='PR '),'疗效评估 (PD or PR or CR)']=1\n",
    "df.loc[(df['疗效评估 (PD or PR or CR)']=='PD')|(df['疗效评估 (PD or PR or CR)']=='pD')|(df['疗效评估 (PD or PR or CR)']=='SD')|(df['疗效评估 (PD or PR or CR)']=='Sd')|(df['疗效评估 (PD or PR or CR)']=='SD '),'疗效评估 (PD or PR or CR)']=0\n",
    "#DCR\n",
    "#df.loc[(df['疗效评估 (PD or PR or CR)']=='CR')|(df['疗效评估 (PD or PR or CR)']=='CR ')|(df['疗效评估 (PD or PR or CR)']=='PR')|(df['疗效评估 (PD or PR or CR)']=='PR ')|(df['疗效评估 (PD or PR or CR)']=='SD')|(df['疗效评估 (PD or PR or CR)']=='SD '),'疗效评估 (PD or PR or CR)']=1\n",
    "#df.loc[(df['疗效评估 (PD or PR or CR)']=='PD')|(df['疗效评估 (PD or PR or CR)']=='PD '),'疗效评估 (PD or PR or CR)']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(np.unique(df['Center'])):\n",
    "    print(f\"{i} has data {np.sum(df['Center']==i)}: HAIC {np.sum((df['Center']==i)&(df['HAIC/TACE']==1))}:{np.sum(df[(df['Center']==i)&(df['HAIC/TACE']==1)]['疗效评估 (PD or PR or CR)'])}, TACE {np.sum((df['Center']==i)&(df['HAIC/TACE']==0))}:{np.sum(df[(df['Center']==i)&(df['HAIC/TACE']==0)]['疗效评估 (PD or PR or CR)'])}\")\n",
    "print(f\"ALL data {len(df['Center'])}: HAIC {np.sum(df['HAIC/TACE']==1)}:{np.sum(df[df['HAIC/TACE']==1]['疗效评估 (PD or PR or CR)'])}, TACE {np.sum(df['HAIC/TACE']==0)}:{np.sum(df[df['HAIC/TACE']==0]['疗效评估 (PD or PR or CR)'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = os.listdir('/home/u20210110/jupyterlab/HAIC_TACE/Problem')\n",
    "index_selected = []\n",
    "for idx in range(len(df)):\n",
    "    vp_name = str(df.iloc[idx, 3])+'_'+os.path.basename(df.iloc[idx, 2])+'_0000.nii.gz_VP'\n",
    "    ap_name = str(df.iloc[idx, 3])+'_'+os.path.basename(df.iloc[idx, 2])+'_0000.nii.gz_AP'\n",
    "    if ((vp_name) in del_list)|((ap_name) in del_list):\n",
    "        index_selected.append(False)\n",
    "    else:\n",
    "        index_selected.append(True)\n",
    "\n",
    "df = df[index_selected]\n",
    "index_selected = []\n",
    "for idx in range(len(df)):\n",
    "    file_name = '/home/u20210110/jupyterlab/HAIC_TACE/Processed_data_cut/'+str(df.iloc[idx, 3])+'/'+os.path.basename(df.iloc[idx, 2])+ '/AP.nii.gz'\n",
    "    if os.path.exists(file_name):\n",
    "        index_selected.append(True)\n",
    "    else:\n",
    "        index_selected.append(False)\n",
    "df = df[index_selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(np.unique(df['Center'])):\n",
    "    print(f\"{i} has data {np.sum(df['Center']==i)}: HAIC {np.sum((df['Center']==i)&(df['HAIC/TACE']==1))}:{np.sum(df[(df['Center']==i)&(df['HAIC/TACE']==1)]['疗效评估 (PD or PR or CR)'])}, TACE {np.sum((df['Center']==i)&(df['HAIC/TACE']==0))}:{np.sum(df[(df['Center']==i)&(df['HAIC/TACE']==0)]['疗效评估 (PD or PR or CR)'])}\")\n",
    "print(f\"ALL data {len(df['Center'])}: HAIC {np.sum(df['HAIC/TACE']==1)}:{np.sum(df[df['HAIC/TACE']==1]['疗效评估 (PD or PR or CR)'])}, TACE {np.sum(df['HAIC/TACE']==0)}:{np.sum(df[df['HAIC/TACE']==0]['疗效评估 (PD or PR or CR)'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[(df['Center']=='SYSUCC')|(df['Center']=='SYSU_first')|(df['Center']=='JNU')]\n",
    "df_val = df[(df['Center']=='CHCAMS')|(df['Center']=='SYSU_third')|(df['Center']=='Gaofei')|(df['Center']=='LUHE')]\n",
    "print(f\"HIAC Train ORR:{df_train['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_train)-df_train['疗效评估 (PD or PR or CR)'].values.sum()}\")\n",
    "print(f\"HIAC Val ORR:{df_val['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_val)-df_val['疗效评估 (PD or PR or CR)'].values.sum()}\")\n",
    "df_train = pd.concat([df_train,df_train[df_train['疗效评估 (PD or PR or CR)']==1],df_train[df_train['疗效评估 (PD or PR or CR)']==1]])\n",
    "print(f\"Ague HIAC Train ORR:{df_train['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_train)-df_train['疗效评估 (PD or PR or CR)'].values.sum()}\")\n",
    "df_val = pd.concat([df_val,df_val[df_val['疗效评估 (PD or PR or CR)']==1],df_val[df_val['疗效评估 (PD or PR or CR)']==1],df_val[df_val['疗效评估 (PD or PR or CR)']==1]])\n",
    "print(f\"Ague HIAC Train ORR:{df_val['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_val)-df_val['疗效评估 (PD or PR or CR)'].values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('TACE_train_20240523.csv',index=False,encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {'train': len(df_train), 'val': len(df_val)}\n",
    "print(f'train: {len(df_train)}, val: {len(df_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset_loader(df_file=df_train,)\n",
    "val_data = Dataset_loader(df_file=df_val,)\n",
    "'''pre_data_loader = {\n",
    "        'train': PrefetchDataLoader(train_data, batch_size=len(df_train), shuffle=False, num_workers=4),\n",
    "        'val': PrefetchDataLoader(val_data, batch_size= len(df_val), shuffle=False, num_workers=4)\n",
    "        }\n",
    "for inputs, labels in pre_data_loader['train']:\n",
    "    train_dataset =  torch.utils.data.TensorDataset(inputs, labels )\n",
    "for inputs, labels  in pre_data_loader['val']:\n",
    "    val_dataset =  torch.utils.data.TensorDataset(inputs, labels )    '''\n",
    "data_set = {'train':train_data,'val':val_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.train import RunConfig\n",
    "from ray.tune import Callback\n",
    "from ray.train import Checkpoint\n",
    "import tempfile \n",
    "\n",
    "from ray.tune.schedulers import ASHAScheduler,AsyncHyperBandScheduler\n",
    "class MyCallback(Callback):\n",
    "    def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "        print(f\"Got {trial} result of {result['training_iteration']}: {result['auc']}\")\n",
    "def stop_fn(trial_id: str, result: dict) -> bool:\n",
    "    return result[\"acc\"] < 0.6 and result[\"training_iteration\"] >= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config,data_set):#config 是一个字典，包含了训练模型所需的各种配置参数，比如模型类型、学习率等。fold 是一个表示交叉验证中的第几折的参数，用于指定当前训练的是哪个数据分组\n",
    "    fold_dir = '/home/u20210110/jupyterlab/HAIC_TACE/2_20240430_finetune_TACE_AP'\n",
    "    \n",
    "    model = VSSM(patch_size1=9*config[\"patch_size\"],patch_size2=config[\"patch_size\"], in_chans=1, num_classes=2, depths=config[\"depths\"], depths_decoder=[2, 9, 2, 2],\n",
    "                 dims=config[\"dims\"], dims_decoder=[768, 384, 192, 96], d_state=config[\"d_state\"], drop_rate=config[\"drop_rate\"], attn_drop_rate=config[\"attn_drop_rate\"], drop_path_rate=config[\"drop_path_rate\"],\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=True)\n",
    "    model = model.to(device)\n",
    "    criterion_train = CELoss_mixup(label_smooth=config['smooth_label'], class_num=2,mix_up=config['mix_up'])#nn.CrossEntropyLoss()#nn.BCEWithLogitsLoss()\n",
    "    criterion_test = CELoss_mixup(label_smooth=config['smooth_label'],class_num=2,mix_up=config['mix_up'])#nn.CrossEntropyLoss()#nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"],betas=(config[\"beta\"], 0.999), eps=config[\"eps\"])\n",
    "    #criterion = nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss()\n",
    "    # Observe that all parameters are being optimized\n",
    "\n",
    "    since = time.time()\n",
    "    num_epochs = config[\"ep\"]\n",
    "    '''\n",
    "    data_loader = {\n",
    "        'train': torch.utils.data.DataLoader(data_set['train'], batch_size=config[\"bs\"], shuffle=True, num_workers=4,pin_memory=True),\n",
    "        'val': torch.utils.data.DataLoader(data_set['val'], batch_size=config[\"bs\"], shuffle=False, num_workers=4,pin_memory=True)}\n",
    "    '''\n",
    "    data_loader = {\n",
    "        'train': PrefetchDataLoader(data_set['train'], batch_size=config[\"bs\"], shuffle=True, num_workers=4),\n",
    "        'val': PrefetchDataLoader(data_set['val'], batch_size= config[\"bs\"], shuffle=False, num_workers=4)\n",
    "        }\n",
    "    \n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    checkpoint = train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch_train = checkpoint_state[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch_train = 0\n",
    "    best_auc = 0.0\n",
    "    best_acc = 0.0\n",
    "    start_epoch_train = 0\n",
    "    n_iter_test = 0\n",
    "    for epoch in range(num_epochs):\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                train_pred = []\n",
    "                test_pred = []\n",
    "                train_label = []\n",
    "                test_label = []\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in data_loader[phase]:\n",
    "                    #sm_labels = label_smooth(labels, 1, config['smooth_label'], is_onehot=True)\n",
    "                    inputs = inputs.to(device)\n",
    "                    sm_labels = labels.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    if phase == 'train' and config['alpha']>0:\n",
    "                        ## mix up\n",
    "                        target = F.one_hot(labels, 2)\t# 转换成one-hot\n",
    "                        alpha = config['alpha']\n",
    "                        lam = np.random.beta(alpha,alpha)\n",
    "                        index = torch.randperm(inputs.size(0)).cuda()\n",
    "                        inputs = lam*inputs+(1-lam)*inputs[index,:]\n",
    "                        sm_labels = lam*sm_labels+(1-lam)*sm_labels[index]\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):#完成了模型在训练和验证阶段的前向传播、损失计算、反向传播和参数更新等步骤。\n",
    "                        outputs,x = model(inputs)\n",
    "                        #outputs = outputs.squeeze()\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        if phase == 'train' and config['alpha']>0:\n",
    "                            loss = criterion_train(outputs, sm_labels)\n",
    "                        else:\n",
    "                            loss1 = criterion_test(outputs, labels.squeeze())\n",
    "                            loss2 = criterion_test(x, labels.squeeze())\n",
    "                            loss = loss1+loss2\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            start_epoch_train+=1\n",
    "                        else:\n",
    "                            n_iter_test+=1\n",
    "                    # statistics#可以实现对模型在训练和验证阶段的损失值和评估指标的实时监控和记录，方便后续分析和可视化。\n",
    "                    if phase == 'train':\n",
    "                        train_pred.extend(nn.Sigmoid()(outputs).cpu().detach().numpy()[:,1])\n",
    "                        train_label.extend(labels.cpu().numpy())\n",
    "                    else:\n",
    "                        test_pred.extend(nn.Sigmoid()(outputs).cpu().detach().numpy()[:,1])\n",
    "                        test_label.extend(labels.cpu().numpy())\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.squeeze().data).cpu().numpy()\n",
    "                    \n",
    "                    \n",
    "                '''   \n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                '''\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "                \n",
    "                if phase == 'val':\n",
    "                    prefix = 'val'\n",
    "                    #print(test_label)\n",
    "                    #print(test_pred)\n",
    "                    epoch_auc = roc_auc_score(test_label,test_pred)\n",
    "                    if epoch_auc > best_auc:\n",
    "                        best_auc = epoch_auc\n",
    "                        torch.save({\"epoch\": epoch, \"model_state\": model.state_dict()}, os.path.join(fold_dir, \"checkpoint.pt\"),)\n",
    "                        train.report(\n",
    "                                    {\"auc\": epoch_auc,\"loss\":epoch_loss},\n",
    "                                    checkpoint=Checkpoint.from_directory(fold_dir)\n",
    "                                    )\n",
    "                        #torch.save(model.state_dict(),f'{fold_dir}/best_model_params.pt')\n",
    "                    else:\n",
    "                        train.report({\"auc\": epoch_auc,\"loss\":epoch_loss})\n",
    "                else:\n",
    "                    prefix = ''\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')#训练时间\n",
    "    print(f'Best val AUC: {best_auc:4f}')#最好的模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "if __name__ == \"__main__\":\n",
    "    fold_dir = '/home/u20210110/jupyterlab/HAIC_TACE/2_20240430_finetune_TACE_AP'\n",
    "    os.makedirs(f'{fold_dir}', exist_ok=True)\n",
    "    train_pred_lists = []\n",
    "    train_label_lists = []\n",
    "    test_pred_lists = []\n",
    "    test_label_lists = []\n",
    "    i = 5\n",
    "    config = {\n",
    "        \"lr\": tune.choice([1e-06]),#tune.loguniform(1e-6, 1e-3),\n",
    "        \"ep\": tune.choice([50]),\n",
    "        \"bs\": 32,\n",
    "        \"beta\": tune.choice([0.9]),#tune.choice([0.1,0.5,0.9]),\n",
    "        \"eps\": tune.choice([1e-08]),#tune.choice([1e-08,1e-06,1e-04,1e-02]),\n",
    "        \"smooth_label\": None,#tune.choice([0,0.1,0.3]),\n",
    "        \"alpha\":tune.choice([0]),#tune.choice([0,0.1,0.5,1,10]),\n",
    "        \"patch_size\": tune.choice([4,9]), \n",
    "        \"depths\":tune.choice([[2, 2, 2, 2],[1,1,1,1],[2,4,4,2]]), \n",
    "        \"dims\": tune.choice([[96, 192, 384, 768],[48,96,192,384],[128,256,512,1024]]), \n",
    "        \"d_state\":tune.choice([16,81]),  \n",
    "        \"drop_rate\":tune.choice([0,0.1,0.3]), \n",
    "        \"attn_drop_rate\":tune.choice([0,0.1,0.3]), \n",
    "        \"drop_path_rate\":tune.choice([0,0.1,0.3]),\n",
    "        \"mix_up\":None,\n",
    "        \n",
    "        }\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    #scheduler = ASHAScheduler()\n",
    "    \n",
    "    scheduler = AsyncHyperBandScheduler(grace_period=5)\n",
    "    '''\n",
    "    reporter = tune.JupyterNotebookReporter(\n",
    "        metric_columns=[\"loss\", \"acc\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_model, dataset=data_set),\n",
    "        resources_per_trial={\"cpu\": 4, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=30,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    '''\n",
    "    #ray.init(ignore_reinit_error=True,num_cpus=4, num_gpus=1, resources={'Custom': 4})\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(tune.with_parameters(train_model, data_set=data_set),\n",
    "            resources={\"cpu\": 4, \"gpu\": 1}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            metric=\"auc\",\n",
    "            mode=\"max\",\n",
    "            num_samples=30,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=train.RunConfig(stop={\"time_total_s\": 10*60},callbacks=[MyCallback()])\n",
    "    )\n",
    "    result = tuner.fit()\n",
    "    best_trial = result.get_best_result(\"auc\", \"max\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial path: {best_trial.path}\")\n",
    "    print(f\"Best trial final validation auc: {best_trial.metrics['auc']}\")\n",
    "    model = VSSM(patch_size1=9*best_trial.config[\"patch_size\"],patch_size2=best_trial.config[\"patch_size\"], in_chans=1, num_classes=2, \n",
    "                 depths=best_trial.config[\"depths\"], depths_decoder=[2, 9, 2, 2],\n",
    "                 dims=best_trial.config[\"dims\"], dims_decoder=[768, 384, 192, 96], \n",
    "                 d_state=best_trial.config[\"d_state\"], \n",
    "                 drop_rate=best_trial.config[\"drop_rate\"], attn_drop_rate=best_trial.config[\"attn_drop_rate\"], drop_path_rate=best_trial.config[\"drop_path_rate\"],\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=True)\n",
    "    model.load_state_dict(torch.load(os.path.join(best_trial.checkpoint.path,'checkpoint.pt'))['model_state'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #开始训练\n",
    "    data_loader = {\n",
    "        'train': torch.utils.data.DataLoader(data_set['train'], batch_size=32, shuffle=False, num_workers=0),\n",
    "        'val': torch.utils.data.DataLoader(data_set['val'], batch_size=32, shuffle=False, num_workers=0)}#创建了两个数据加载器，用于加载训练集和验证集的数据\n",
    "    train_pred_list = []\n",
    "    train_label_list = []                              \n",
    "    for inputs, labels in data_loader['train']:\n",
    "        inputs,_ = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        #outputs = outputs.squeeze()\n",
    "        outputs = nn.Sigmoid()(outputs)\n",
    "        train_pred_list.extend(outputs.cpu().detach().numpy()[:,1])\n",
    "        train_label_list.extend(labels.cpu().numpy())                                                       \n",
    "    df_train['pred1'] = train_pred_list\n",
    "    df_train.to_csv(f'{fold_dir}/train_{i}.csv')\n",
    "    train_pred_lists.append(train_pred_list)\n",
    "    train_label_lists.append(train_label_list)                            \n",
    "    test_pred_list = []\n",
    "    test_label_list = []\n",
    "    for inputs, labels in data_loader['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "       # outputs = outputs.squeeze()\n",
    "        outputs = nn.Sigmoid()(outputs)\n",
    "        test_pred_list.extend(outputs.cpu().detach().numpy()[:,1])\n",
    "        test_label_list.extend(labels.cpu().numpy())                   \n",
    "    df_val['pred1'] = test_pred_list\n",
    "    df_val.to_csv(f'{fold_dir}/val_{i}.csv')\n",
    "    test_pred_lists.append(test_pred_list)\n",
    "    test_label_lists.append(test_label_list)   \n",
    "    # PR_plot_CV(np.concatenate([train_pred_list,1-np.array(train_pred_list)]),train_label_list,'train',fold_dir)\n",
    "    # PR_plot_CV(np.concatenate([test_pred_list,1-test_pred_list]),test_label_list,'test',fold_dir)\n",
    "\n",
    "    #设置绘图\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95,\n",
    "                    hspace=0.2, wspace=0.2)\n",
    "    parameters = {'lw': 2, 'alpha_fold': 0.3, 'alpha_ave': 0.8, 'fontsize': 20}\n",
    "\n",
    "    title = f'Train Cohort'\n",
    "    ax[0] = binary_auc_plot_cv(ax[0], train_pred_lists, train_label_lists, parameters, title)\n",
    "    title = f'Test Cohort '\n",
    "    ax[1] = binary_auc_plot_cv(ax[1], test_pred_lists, test_label_lists, parameters, title)\n",
    "    fig.savefig(f'{fold_dir}/ROC_CV.png', dpi=200, bbox_inches='tight')\n",
    "    #print(f\"Model structure: {model}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fold_dir = '/home/u20210110/jupyterlab/HAIC_TACE/20240430_finetune_AP'\n",
    "    os.makedirs(f'{fold_dir}', exist_ok=True)\n",
    "    train_pred_lists = []\n",
    "    train_label_lists = []\n",
    "    test_pred_lists = []\n",
    "    test_label_lists = []\n",
    "    i = 5\n",
    "    config = {\n",
    "        \"lr\": tune.loguniform(1e-7, 1e-4),\n",
    "        \"ep\": tune.choice([50,100,200]),\n",
    "        \"bs\": tune.choice([32]),\n",
    "        \"beta\": tune.choice([0.5,0.1,0.9]),\n",
    "        \"eps\": tune.choice([1e-08,1e-06,1e-04,1e-02]),\n",
    "        \"smooth_label\": tune.choice([0.,0.1,0.3]),\n",
    "        \"alpha\":tune.choice([0,0.1,0.5,1,5,10]),\n",
    "        \"patch_size\": 9, \n",
    "        \"depths\":[2, 4, 4, 2], \n",
    "        \"dims\":[128, 256, 512, 1024],\n",
    "        \"d_state\":16,  \n",
    "        \"drop_rate\":0.1, \n",
    "        \"attn_drop_rate\":0.1, \n",
    "        \"drop_path_rate\":0.3,\n",
    "        }\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    #scheduler = ASHAScheduler()\n",
    "    scheduler = AsyncHyperBandScheduler(grace_period=5)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model,data_set = data_set),\n",
    "            resources={\"cpu\": 2, \"gpu\": 1}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            metric=\"auc\",\n",
    "            mode=\"max\",\n",
    "            num_samples=100,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=train.RunConfig(stop={\"time_total_s\": 20*60},callbacks=[MyCallback()])\n",
    "    )\n",
    "    result = tuner.fit()\n",
    "    best_trial = result.get_best_result(\"auc\", \"max\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial path: {best_trial.path}\")\n",
    "    print(f\"Best trial final validation auc: {best_trial.metrics['auc']}\")\n",
    "    \n",
    "    model = medmamba(patch_size1=9*best_trial.config[\"patch_size\"],patch_size2=best_trial.config[\"patch_size\"], in_chans=1, num_classes=1, \n",
    "                 depths=best_trial.config[\"depths\"], depths_decoder=[2, 9, 2, 2],\n",
    "                 dims=best_trial.config[\"dims\"], dims_decoder=[768, 384, 192, 96], \n",
    "                 d_state=best_trial.config[\"d_state\"], \n",
    "                 drop_rate=best_trial.config[\"drop_rate\"], attn_drop_rate=best_trial.config[\"attn_drop_rate\"], drop_path_rate=best_trial.config[\"drop_path_rate\"],\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=True)\n",
    "    model.load_state_dict(torch.load(os.path.join(best_trial.checkpoint.path,'checkpoint.pt'))['model_state'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #开始训练\n",
    "    data_loader = {\n",
    "        'train': torch.utils.data.DataLoader(data_set['train'], batch_size=32, shuffle=False, num_workers=0),\n",
    "        'val': torch.utils.data.DataLoader(data_set['val'], batch_size=32, shuffle=False, num_workers=0)}#创建了两个数据加载器，用于加载训练集和验证集的数据\n",
    "    train_pred_list = []\n",
    "    train_label_list = []                              \n",
    "    for inputs, labels in data_loader['train']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        #outputs = outputs.squeeze()\n",
    "        outputs = nn.Sigmoid()(outputs)\n",
    "        train_pred_list.extend(outputs.cpu().detach().numpy())\n",
    "        train_label_list.extend(labels.cpu().numpy())                                                       \n",
    "    df_train['pred1'] = train_pred_list\n",
    "    df_train.to_csv(f'{fold_dir}/train_{i}.csv')\n",
    "    train_pred_lists.append(train_pred_list)\n",
    "    train_label_lists.append(train_label_list)                            \n",
    "    test_pred_list = []\n",
    "    test_label_list = []\n",
    "    for inputs, labels in data_loader['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        #outputs = outputs.squeeze()\n",
    "        outputs = nn.Sigmoid()(outputs)\n",
    "        test_pred_list.extend(outputs.cpu().detach().numpy())\n",
    "        test_label_list.extend(labels.cpu().numpy())                   \n",
    "    df_val['pred1'] = test_pred_list\n",
    "    df_val.to_csv(f'{fold_dir}/val_{i}.csv')\n",
    "    test_pred_lists.append(test_pred_list)\n",
    "    test_label_lists.append(test_label_list)   \n",
    "    # PR_plot_CV(np.concatenate([train_pred_list,1-np.array(train_pred_list)]),train_label_list,'train',fold_dir)\n",
    "    # PR_plot_CV(np.concatenate([test_pred_list,1-test_pred_list]),test_label_list,'test',fold_dir)\n",
    "\n",
    "    #设置绘图\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95,\n",
    "                    hspace=0.2, wspace=0.2)\n",
    "    parameters = {'lw': 2, 'alpha_fold': 0.3, 'alpha_ave': 0.8, 'fontsize': 20}\n",
    "\n",
    "    title = f'Train Cohort'\n",
    "    ax[0] = binary_auc_plot_cv(ax[0], train_pred_lists, train_label_lists, parameters, title)\n",
    "    title = f'Test Cohort '\n",
    "    ax[1] = binary_auc_plot_cv(ax[1], test_pred_lists, test_label_lists, parameters, title)\n",
    "    fig.savefig(f'{fold_dir}/ROC_CV.png', dpi=200, bbox_inches='tight')\n",
    "    #print(f\"Model structure: {model}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "for result_single in result:\n",
    "    label = f\"lr={result_single.config['lr']}\"#:.3f\n",
    "    if ax is None:\n",
    "        ax = result_single.metrics_dataframe.plot(\"training_iteration\", \"auc\", label=label)\n",
    "    else:\n",
    "        result_single.metrics_dataframe.plot(\"training_iteration\", \"auc\", ax=ax, label=label)\n",
    "ax.set_title(\"AUC vs. Training Iteration for All Trials\")\n",
    "ax.set_ylabel(\"AUC\")\n",
    "ax.legend(loc=\"lower right\", bbox_to_anchor=(1.05,1.1),borderaxespad = 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tune again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma = 2, alpha = 1, size_average = True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "        self.elipson = 0.000001\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        cal culates loss\n",
    "        logits: batch_size * labels_length * seq_length\n",
    "        labels: batch_size * seq_length\n",
    "        \"\"\"\n",
    "        if labels.dim() > 2:\n",
    "            labels = labels.contiguous().view(labels.size(0),  -1)\n",
    "            #labels = labels.transpose(1, 2)\n",
    "            labels = labels.contiguous().view(-1, labels.size(2)).squeeze()\n",
    "        if logits.dim() > 3:\n",
    "            logits = logits.contiguous().view(logits.size(0), logits.size(1),  -1)\n",
    "            #logits = logits.transpose(2, 3)\n",
    "            logits = logits.contiguous().view(-1, logits.size(1), logits.size(2)).squeeze()\n",
    "        assert(logits.size(0) == labels.size(0))\n",
    "        #assert(logits.size(2) == labels.size(1))\n",
    "        batch_size = logits.size(0)\n",
    "        labels_length = logits.size(1)\n",
    "        #seq_length = logits.size(2)\n",
    "\n",
    "        # transpose labels into labels onehot\n",
    "        new_label = labels.unsqueeze(1)\n",
    "        #print(torch.zeros([batch_size, labels_length]))\n",
    "        label_onehot = torch.zeros([batch_size, labels_length]).to(device).scatter_(1, new_label,1)\n",
    "\n",
    "        # calculate log\n",
    "        log_p = F.log_softmax(logits,dim=1)\n",
    "        pt = label_onehot * log_p\n",
    "        sub_pt = 1 - pt\n",
    "        fl = -self.alpha * (sub_pt)**self.gamma * log_p\n",
    "        if self.size_average:\n",
    "            return fl.mean()\n",
    "        else:\n",
    "            return fl.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_again_model(config, dataset):#config 是一个字典，包含了训练模型所需的各种配置参数，比如模型类型、学习率等。fold 是一个表示交叉验证中的第几折的参数，用于指定当前训练的是哪个数据分组\n",
    "    model = VSSM(patch_size1=9*config[\"patch_size\"],patch_size2=config[\"patch_size\"], in_chans=1, num_classes=2, depths=config[\"depths\"], depths_decoder=[2, 9, 2, 2],\n",
    "                 dims=config[\"dims\"], dims_decoder=[768, 384, 192, 96], d_state=config[\"d_state\"], drop_rate=config[\"drop_rate\"], attn_drop_rate=config[\"attn_drop_rate\"], drop_path_rate=config[\"drop_path_rate\"],\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"],betas=(config[\"beta\"], 0.999), eps=config[\"eps\"])\n",
    "    criterion_train = CELoss_mixup(label_smooth=config['smooth_label'],class_num=2,mix_up=None)#nn.CrossEntropyLoss()#nn.BCEWithLogitsLoss()\n",
    "    criterion_test = CELoss_mixup(label_smooth=config['smooth_label'],class_num=2,mix_up=None)#nn.CrossEntropyLoss()#nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    num_epochs = config[\"ep\"]\n",
    "\n",
    "    since = time.time()\n",
    "    \n",
    "    writer = SummaryWriter(comment=f'LR_{config[\"lr\"]}_BS_{config[\"bs\"]}')\n",
    "                         \n",
    "    data_loader = {\n",
    "        'train': torch.utils.data.DataLoader(dataset['train'], batch_size=config[\"bs\"], shuffle=True, num_workers=4,pin_memory=True),\n",
    "        'val': torch.utils.data.DataLoader(dataset['val'], batch_size=config[\"bs\"], shuffle=False, num_workers=4,pin_memory=True)}\n",
    "\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    \n",
    "    #best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "    \n",
    "    if config[\"train\"]==True:\n",
    "        model.load_state_dict(torch.load(f'{fold_dir}/best_model_params.pt'))\n",
    "    ep_loss = PlotLosses()\n",
    "    #torch.save(model.state_dict(), best_model_params_path)\n",
    "    best_acc = 0.0\n",
    "    best_auc = 0.0\n",
    "    start_epoch_train = 0\n",
    "    n_iter_test = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}                    \n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        with tqdm(total=len(data_loader['train'])+1) as pbar:\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                epoch_auc = 0\n",
    "                train_pred = []\n",
    "                test_pred = []\n",
    "                train_label = []\n",
    "                test_label = []        \n",
    "                # Iterate over data.\n",
    "                for inputs, labels in data_loader[phase]:\n",
    "                    #sm_labels = label_smooth(labels, 1, config['smooth_label'], is_onehot=True)\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    if phase == 'train' and config['alpha']>0:\n",
    "                        ## mix up\n",
    "                        target = F.one_hot(labels, 2)\t# 转换成one-hot\n",
    "                        alpha = config['alpha']\n",
    "                        lam = np.random.beta(alpha,alpha)\n",
    "                        index = torch.randperm(inputs.size(0)).cuda()\n",
    "                        inputs = lam*inputs+(1-lam)*inputs[index,:]\n",
    "                        sm_labels = lam*target+(1-lam)*target[index]\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):#完成了模型在训练和验证阶段的前向传播、损失计算、反向传播和参数更新等步骤。\n",
    "                        outputs,x = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        if phase == 'train' and config['alpha']>0:\n",
    "                            loss = criterion_train(outputs, sm_labels)\n",
    "                        else:\n",
    "                           loss1 = criterion_test(outputs, labels.squeeze())\n",
    "                           loss2 = criterion_test(x, labels.squeeze())\n",
    "                           loss = loss1+loss2\n",
    "                        #loss2 = criterion(preds.squeeze().float(), sm_labels.squeeze().float())\n",
    "                        #loss = 0.1*loss1+loss2\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    \n",
    "                    # statistics#可以实现对模型在训练和验证阶段的损失值和评估指标的实时监控和记录，方便后续分析和可视化。\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.squeeze().data).cpu().numpy()  #torch.sum((nn.Sigmoid()(outputs)>0.5) == (labels.data>0.5)).item()\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        train_pred.extend(nn.Softmax(dim=1)(outputs)[:,1].cpu().detach().numpy())\n",
    "                        train_label.extend(labels.squeeze().cpu().numpy())\n",
    "                    else:\n",
    "                        test_pred.extend(nn.Softmax(dim=1)(outputs)[:,1].cpu().detach().numpy())\n",
    "                        test_label.extend(labels.squeeze().cpu().numpy())\n",
    "                    if phase == 'train':\n",
    "                        prefix = ''\n",
    "                        auc_score = roc_auc_score(train_label,train_pred)\n",
    "                        writer.add_scalar('Loss/train', loss.item() * inputs.size(0) / config[\"bs\"], start_epoch_train)\n",
    "                        writer.add_scalar('ACC/train', running_corrects/ (config[\"bs\"]*(start_epoch_train+1)), start_epoch_train)\n",
    "                        writer.add_scalar('AUC/test', auc_score, start_epoch_train)\n",
    "                        start_epoch_train += 1\n",
    "                    elif phase == 'val':\n",
    "                        prefix = 'val_'\n",
    "                        auc_score = roc_auc_score(test_label,test_pred)\n",
    "                        writer.add_scalar('Loss/test', loss.item() * inputs.size(0) / config[\"bs\"], n_iter_test)\n",
    "                        writer.add_scalar('ACC/test', running_corrects/ (config[\"bs\"]*(n_iter_test+1)), n_iter_test)\n",
    "                        writer.add_scalar('AUC/test', auc_score, n_iter_test)\n",
    "                        n_iter_test += 1\n",
    "\n",
    "                    pbar.set_description(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(loss = loss.item(),acc =running_corrects/(config[\"bs\"]*(start_epoch_train+1)),auc=auc_score)\n",
    "                #if phase == 'train':\n",
    "                #    scheduler.step()\n",
    "                    \n",
    "                \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "                if phase == 'val':\n",
    "                    prefix = 'val_'\n",
    "                    epoch_auc = roc_auc_score(test_label,test_pred)\n",
    "                    pbar.set_postfix(val_loss = epoch_loss,val_acc=epoch_acc,val_auc=epoch_auc)\n",
    "                    if epoch_auc > best_auc:\n",
    "                        best_auc = epoch_auc\n",
    "                        #torch.save(model.state_dict(),f'{fold_dir}/best_model_params_{epoch:04d}_{best_acc:.4f}.pt')\n",
    "                        torch.save(model.state_dict(),f'{fold_dir}/best_model_params.pt')\n",
    "                else:\n",
    "                    epoch_auc = roc_auc_score(train_label,train_pred)\n",
    "                    prefix = ''\n",
    "                    pbar.set_postfix(loss = epoch_loss,acc=epoch_acc,auc=epoch_auc)\n",
    "                logs[prefix + 'loss'] = epoch_loss  \n",
    "                logs[prefix + 'ACC'] = epoch_acc\n",
    "                logs[prefix + 'AUC'] = epoch_auc\n",
    "                \n",
    "        ep_loss.update(logs)\n",
    "        ep_loss.send()\n",
    "    writer.flush()\n",
    "    writer.close()                                      \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')#训练时间\n",
    "    print(f'Best val AUC: {best_auc:4f}')#最好的模型\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(torch.load(f'{fold_dir}/best_model_params.pt'))\n",
    "    #torch.save(model.state_dict(), best_model_params_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[(df['Center']=='SYSUCC')|(df['Center']=='SYSU_first')|(df['Center']=='JNU')]\n",
    "df_val = df[(df['Center']=='CHCAMS')|(df['Center']=='SYSU_third')|(df['Center']=='Gaofei')|(df['Center']=='LUHE')]\n",
    "print(f\"HIAC Train ORR:{df_train['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_train)-df_train['疗效评估 (PD or PR or CR)'].values.sum()}\")\n",
    "print(f\"HIAC Val ORR:{df_val['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_val)-df_val['疗效评估 (PD or PR or CR)'].values.sum()}\")\n",
    "df_train = pd.concat([df_train,df_train[df_train['疗效评估 (PD or PR or CR)']==1],df_train[df_train['疗效评估 (PD or PR or CR)']==1]])\n",
    "print(f\"Ague HIAC Train ORR:{df_train['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_train)-df_train['疗效评估 (PD or PR or CR)'].values.sum()}\")\n",
    "df_val = pd.concat([df_val,df_val[df_val['疗效评估 (PD or PR or CR)']==1],df_val[df_val['疗效评估 (PD or PR or CR)']==1],df_val[df_val['疗效评估 (PD or PR or CR)']==1],df_val[df_val['疗效评估 (PD or PR or CR)']==1][:13]])\n",
    "print(f\"Ague HIAC Train ORR:{df_val['疗效评估 (PD or PR or CR)'].values.sum()}, Non-ORR:{len(df_val)-df_val['疗效评估 (PD or PR or CR)'].values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset_loader(df_file=df_train,)\n",
    "val_data = Dataset_loader(df_file=df_val,)\n",
    "pre_data_loader = {\n",
    "        'train': PrefetchDataLoader(train_data, batch_size=len(df_train), shuffle=False, num_workers=4),\n",
    "        'val': PrefetchDataLoader(val_data, batch_size= len(df_val), shuffle=False, num_workers=4)\n",
    "        }\n",
    "for inputs, labels in pre_data_loader['train']:\n",
    "    train_dataset =  torch.utils.data.TensorDataset(inputs, labels )\n",
    "'''with tqdm(total=len(inputs)) as pbar:\n",
    "    single = 0\n",
    "    for single_image,single_label in zip(inputs,labels):\n",
    "        single_image = Image.fromarray((single_image.cpu().detach().numpy()[0,:,:]*255).astype('uint8'))\n",
    "        os.makedirs(f'/home/u20210110/jupyterlab/HAIC_TACE/Check_train_{single_label.cpu().numpy()[0]}',exist_ok =True)\n",
    "        single_image.save(f'/home/u20210110/jupyterlab/HAIC_TACE/Check_train_{single_label.cpu().numpy()[0]}/'+str(single)+'.jpg')\n",
    "        single += 1\n",
    "        pbar.update(1)'''\n",
    "for inputs, labels  in pre_data_loader['val']:\n",
    "    val_dataset =  torch.utils.data.TensorDataset(inputs, labels )\n",
    "'''with tqdm(total=len(inputs)) as pbar:\n",
    "    single = 0\n",
    "    for single_image,single_label in zip(inputs,labels):\n",
    "        single_image = Image.fromarray((single_image.cpu().detach().numpy()[0,:,:]*255).astype('uint8'))\n",
    "        os.makedirs(f'/home/u20210110/jupyterlab/HAIC_TACE/Check_val_{single_label.cpu().numpy()[0]}',exist_ok =True)\n",
    "        single_image.save(f'/home/u20210110/jupyterlab/HAIC_TACE/Check_val_{single_label.cpu().numpy()[0]}/'+str(single)+'.jpg')\n",
    "        single+=1\n",
    "        pbar.update(1)'''\n",
    "data_set = {'train':train_dataset,'val':val_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fold_dir = '/home/u20210110/jupyterlab/HAIC_TACE/2_final_TACE_AP'\n",
    "    os.makedirs(f'{fold_dir}', exist_ok=True)\n",
    "    train_pred_lists = []\n",
    "    train_label_lists = []\n",
    "    test_pred_lists = []\n",
    "    test_label_lists = []\n",
    "    config = {\n",
    "        'lr': 1e-06,\n",
    "        'ep': 200,\n",
    "        'bs': 32,\n",
    "        'beta': 0.5,\n",
    "        'eps': 0.01,\n",
    "        'smooth_label':None,\n",
    "        'alpha': 0,#10,\n",
    "        'patch_size': 9,\n",
    "        'depths': [2, 4, 4, 2],\n",
    "        'dims': [128, 256, 512, 1024],\n",
    "        'd_state': 16,\n",
    "        'drop_rate': 0.1,\n",
    "        'attn_drop_rate': 0.1,\n",
    "        'drop_path_rate': 0.3,\n",
    "        'train':False,\n",
    "        }\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    model = train_again_model(config, data_set)\n",
    "    \n",
    "    #开始训练\n",
    "    data_loader = {\n",
    "        'train': torch.utils.data.DataLoader(data_set['train'], batch_size=32, shuffle=False, num_workers=0),\n",
    "        'val': torch.utils.data.DataLoader(data_set['val'], batch_size=32, shuffle=False, num_workers=0)}#创建了两个数据加载器，用于加载训练集和验证集的数据\n",
    "    train_pred_list = []\n",
    "    train_label_list = []                              \n",
    "    for inputs, labels in data_loader['train']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "        #outputs = outputs.squeeze()\n",
    "        outputs = nn.Softmax(dim=1)(outputs)\n",
    "        train_pred_list.extend(outputs.cpu().detach().numpy()[:,1])\n",
    "        train_label_list.extend(labels.cpu().numpy())                                                       \n",
    "    df_train['pred1'] = train_pred_list\n",
    "    df_train.to_csv(f'{fold_dir}/train_{i}.csv')\n",
    "    train_pred_lists.append(train_pred_list)\n",
    "    train_label_lists.append(train_label_list)                            \n",
    "    test_pred_list = []\n",
    "    test_label_list = []\n",
    "    for inputs, labels in data_loader['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "        #outputs = outputs.squeeze()\n",
    "        outputs = nn.Softmax(dim=1)(outputs)\n",
    "        test_pred_list.extend(outputs.cpu().detach().numpy()[:,1])\n",
    "        test_label_list.extend(labels.cpu().numpy())                   \n",
    "    df_val['pred1'] = test_pred_list\n",
    "    df_val.to_csv(f'{fold_dir}/val_{i}.csv')\n",
    "    test_pred_lists.append(test_pred_list)\n",
    "    test_label_lists.append(test_label_list)   \n",
    "    # PR_plot_CV(np.concatenate([train_pred_list,1-np.array(train_pred_list)]),train_label_list,'train',fold_dir)\n",
    "    # PR_plot_CV(np.concatenate([test_pred_list,1-test_pred_list]),test_label_list,'test',fold_dir)\n",
    "\n",
    "    #设置绘图\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95,\n",
    "                    hspace=0.2, wspace=0.2)\n",
    "    parameters = {'lw': 2, 'alpha_fold': 0.3, 'alpha_ave': 0.8, 'fontsize': 20}\n",
    "\n",
    "    title = f'Train Cohort'\n",
    "    ax[0] = binary_auc_plot_cv(ax[0], train_pred_lists, train_label_lists, parameters, title)\n",
    "    title = f'Test Cohort '\n",
    "    ax[1] = binary_auc_plot_cv(ax[1], test_pred_lists, test_label_lists, parameters, title)\n",
    "    fig.savefig(f'{fold_dir}/ROC_CV.png', dpi=200, bbox_inches='tight')\n",
    "    #print(f\"Model structure: {model}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "\n",
    "def cut_roi(data):\n",
    "    \n",
    "    silce_list = []\n",
    "    for silce in range(len(data)):\n",
    "        if np.sum(data[silce])>0:\n",
    "            silce_list.append(silce)\n",
    "    y_list = []\n",
    "    for y in range((data).shape[1]):\n",
    "        if np.sum(data[:,y,:])>0:\n",
    "            y_list.append(y)\n",
    "    x_list = []\n",
    "    for x in range((data).shape[1]):\n",
    "        if np.sum(data[:,:,x])>0:\n",
    "            x_list.append(x)\n",
    "    if (len(silce_list)>0)&(len(y_list)>0)&(len(x_list)>0):\n",
    "        return silce_list[0],silce_list[-1],y_list[0],y_list[-1],x_list[0],x_list[-1]\n",
    "    else:\n",
    "        return False,False,False,False,False,False\n",
    "phase_str = 'AP'\n",
    "files_dir = '/home/u20210110/jupyterlab/HAIC_TACE/All_data/'+phase_str\n",
    "center = 40\n",
    "width = 350\n",
    "depth = 9\n",
    "patients_id = os.listdir(files_dir)#VP\n",
    "wrong_list = []\n",
    "df_val = df[(df['Center']=='CHCAMS')|(df['Center']=='SYSU_third')|(df['Center']=='Gaofei')|(df['Center']=='LUHE')]\n",
    "with tqdm(total=len(patients_id)) as pbar:\n",
    "    for single in patients_id:\n",
    "        if  ('CHCAMS' in single) or ('SYSU_third' in single) or ('Gaofei' in single) or ('SYSU_third' in single) or ('LUHE' in single):\n",
    "            ids = single.split('_')[-2]\n",
    "            center_dir = single.replace('_'+ids+'_0000.nii.gz','')\n",
    "            full_save_dir = os.path.join(files_dir.replace('All_data/'+phase_str,'Processed_data_cut_nnUnet'),center_dir,ids)\n",
    "            os.makedirs(full_save_dir,exist_ok=True)\n",
    "            phase_data = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(files_dir,single)))\n",
    "            phase_mask = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(files_dir.replace('All_data/'+phase_str,'nnUNet/output_all_'+phase_str),single.replace('_0000',''))))\n",
    "            phase_mask[phase_mask==1] = 0\n",
    "            z_min,z_max,_,_,_,_ = cut_roi(phase_mask)\n",
    "            phase_mask = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(files_dir.replace('All_data/'+phase_str,'nnUNet/output_all_'+phase_str),single.replace('_0000',''))))\n",
    "            phase_mask[phase_mask>=1] = 255\n",
    "            _,_,y_min,y_max,x_min,x_max = cut_roi(phase_mask)\n",
    "            #import pdb;pdb.set_trace()\n",
    "            if z_min&z_max&y_min&y_max&x_min&x_max:\n",
    "                # 转换成窗宽窗位\n",
    "                min_ = (2 * center - width) / 2.0 + 0.5\n",
    "                max_ = (2 * center + width) / 2.0 + 0.5\n",
    "                dFactor = 255.0 / (max_ - min_)\n",
    "                phase_data = phase_data - min_\n",
    "                phase_data = np.trunc(phase_data * dFactor)\n",
    "                phase_data[phase_data < 0.0] = 0\n",
    "                phase_data[phase_data > 255.0] = 255  # 转换为窗位窗位之后的数据\n",
    "                roi_data = (phase_data*(phase_mask/255).astype('uint8'))[z_min:z_max+1,:,:]#y_min:y_max,x_min:x_max]\n",
    "                #roi_data = (phase_data.astype('uint8'))[z_min:z_max,y_min:y_max,x_min:x_max]\n",
    "                roi = sitk.GetImageFromArray(roi_data)\n",
    "                sitk.WriteImage(roi,os.path.join(full_save_dir,phase_str+'.nii.gz'))\n",
    "                #idx_slice = len(roi_data) // 2\n",
    "                #image = roi_data[idx_slice] \n",
    "                steps = len(roi_data)//depth\n",
    "                if steps>0:\n",
    "                    if np.mod(len(roi_data),depth)==0:\n",
    "                        roi_data = roi_data[::steps,:,:]\n",
    "                    else:\n",
    "                        roi_data = roi_data[steps::steps,:,:]\n",
    "                image = resize(roi_data,(depth,224,224))\n",
    "                image = image.reshape(depth*224,224)\n",
    "                image = Image.fromarray(image.astype('uint8'))\n",
    "                os.makedirs('/home/u20210110/jupyterlab/HAIC_TACE/Check',exist_ok =True)\n",
    "                image.save('/home/u20210110/jupyterlab/HAIC_TACE/Check/'+str(single)+'_'+phase_str+'.jpg')\n",
    "            else:\n",
    "                wrong_list.append(single)\n",
    "        pbar.update(1)\n",
    "pd.DataFrame(wrong_list).to_csv('/home/u20210110/jupyterlab/HAIC_TACE/All_data/'+phase_str+'_wronglist.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
